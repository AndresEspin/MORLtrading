{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/2aPuee83l9KMCxzucNrk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9z6b0l1jZXi4"},"outputs":[],"source":["#%%writefile morl_module.py\n","\n","import os\n","from datetime import datetime, timedelta\n","from typing import List, Optional, Type\n","\n","import gymnasium as gym\n","from gymnasium import spaces\n","from gymnasium.wrappers import TimeLimit\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import optuna\n","import torch as th\n","import torch.nn as nn\n","import yfinance as yf\n","\n","from stable_baselines3 import A2C, PPO\n","from stable_baselines3.common.callbacks import BaseCallback, CallbackList, CheckpointCallback, EvalCallback\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.evaluation import evaluate_policy\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.policies import ActorCriticPolicy\n","from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n","from stable_baselines3.common.utils import set_random_seed\n","from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, VecNormalize\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import pickle\n","\n","\n","# ------------- 1. Carga de datos y log-retornos -----------\n","def load_price_data(\n","    start_date: str = \"2009-01-01\",\n","    end_date: str = None,\n","    window_size: int= 5,\n","    dataset: pd.DataFrame = pd.DataFrame()\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Filtra fechas, calcula log-retornos diarios normalizados y genera ventana.\n","    Mantiene precios adj_close_ para la simulación financiera.\n","    \"\"\"\n","    start_dt = pd.to_datetime(start_date)\n","    end_dt = pd.to_datetime(end_date or datetime.today())\n","    df = dataset.copy()\n","\n","    # Identificar columnas de precio ajustado\n","    price_cols = df.filter(regex=r\"^adj_close_\").columns\n","\n","    # Calcular log-retornos normalizados: log(p_t / p_{t-1}) / 0.02\n","    for col in price_cols:\n","        ret_col = col.replace(\"adj_close_\", \"logret_\")\n","        df[ret_col] = np.log(df[col] / df[col].shift(1)).fillna(0) / 0.02\n","        # Generar ventana de log-retornos\n","        for lag in range(1, window_size+1):\n","            lag_col = f\"{ret_col}_lag_{lag}\"\n","            df[lag_col] = df[ret_col].shift(lag).fillna(0)\n","\n","    # Normalizar ESG:\n","    for col in df.filter(regex=r\"^esg_score_\").columns:\n","        df[col] = df[col] / 100.0\n","\n","    # Filtrado por fechas\n","    df['date'] = pd.to_datetime(df['date'])\n","    df = df[(df['date'] >= start_dt) & (df['date'] <= end_dt)].reset_index(drop=True)\n","    return df.set_index('date').sort_index()\n","\n","# ------------- 2. Entorno MOTradingEnv ---------------------------------\n","\n","class MOTradingEnv(gym.Env):\n","    \"\"\"\n","    Entorno multi-activo para trading de acciones.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        data_df: np.ndarray,\n","        tickers: list[str],\n","        cash: float = 1e6,\n","        fee: float = 0.001,\n","        max_episode_length: Optional[int] = None,\n","        base_seed: int = 42,\n","        random_start: bool = False,\n","        warmup_episodes: int = 100,\n","        threshold_norentable: float = 0.01,\n","        threshold_oportunidad: float = 0.02,\n","        penalty_factor: float = 0.2\n","    ):\n","        super().__init__()\n","        # Parámetros básicos\n","        self.df = data_df\n","        self.tickers = tickers\n","        self.n = len(tickers)\n","        self.cash0 = cash\n","        self.fee = fee\n","        self.max_len = max_episode_length\n","        self.base_seed = base_seed\n","        self.random_start = random_start\n","        self.warmup_episodes = warmup_episodes\n","        self.threshold_norentable = threshold_norentable\n","        self.threshold_oportunidad = threshold_oportunidad\n","        self.penalty_factor = penalty_factor\n","        self.episode_count = 0\n","        self.reward_dim = 3\n","\n","        # Memorias para backtest detallado\n","        self.actions_memory = []    # lista de arrays de acciones por paso\n","        self.reward_vec_memory = [] # lista de np.array ([r0, r1, r2])\n","        self.asset_memory = []    # lista de asset value por paso\n","        self.date_memory = []    # lista de timestamps por paso\n","\n","        # Espacio de acciones y observaciones\n","        self.action_space = spaces.MultiDiscrete([5]*self.n)\n","        n_logret = self.df.filter(regex=r\"^logret_\").shape[1]\n","        n_tech = self.df.filter(regex=r\"^tecn_\").shape[1]\n","        n_fund = self.df.filter(regex=r\"^fund_\").shape[1]\n","        n_div = self.df.filter(regex=r\"^dividendo_\").shape[1]\n","        n_esg = self.df.filter(regex=r\"^esg_score_\").shape[1]\n","        n_macro = self.df.filter(regex=r\"^macro_\").shape[1]\n","        obs_dim = n_logret + n_tech + n_fund + n_div + n_esg + n_macro + self.n + 1\n","        self.observation_space = spaces.Box(\n","            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32\n","        )\n","\n","        # Precaálculo d einidcadores para warm-up\n","        for t in self.tickers:\n","            price = self.df[f\"adj_close_{t}\"]\n","\n","            # Medias móviles\n","            self.df[f\"ma10_{t}\"] = price.rolling(10).mean()\n","            self.df[f\"ma50_{t}\"] = price.rolling(50).mean()\n","\n","            # Bollinger bands 20 días\n","            mb20 = price.rolling(20).mean()\n","            std20 = price.rolling(20).std()\n","            self.df[f\"bb_up_{t}\"] = mb20 + 2*std20\n","            self.df[f\"bb_dn_{t}\"] = mb20 - 2*std20\n","\n","    def reset(self, seed=None, options=None):\n","        # Semilla y posición inicial aleatoria\n","        seed = (self.base_seed + self.episode_count) if seed is None else seed\n","        np.random.seed(seed)\n","        if self.random_start:\n","            upper = len(self.df) - (self.max_len or len(self.df)) + 1\n","            self.start = np.random.randint(0, max(1, upper))\n","        else:\n","            self.start = 0\n","\n","        self.ptr = self.start\n","        self.episode_count += 1\n","\n","        # Capital inicial repartido 50/50\n","        half_cash = 0.5 * self.cash0\n","        half_inv = 0.5 * self.cash0\n","\n","        # Precios en inicio\n","        row0 = self.df.iloc[self.start]\n","        prices = np.array([row0[f\"adj_close_{t}\"] for t in self.tickers], dtype=np.float32)\n","\n","        # Cantidad de cada acción\n","        per_asset = half_inv / self.n\n","        self.port = (per_asset / (prices * (1 + self.fee))).astype(np.float32)\n","\n","        # Cash restante\n","        self.cash = half_cash\n","\n","        # Inicializar memorias\n","        self.history = [ half_cash + half_inv ]\n","        self.asset_memory = [ half_cash + half_inv ]\n","        self.date_memory = [ self.df.index[self.start] ]\n","        self.actions_memory = []\n","        self.reward_vec_memory = []\n","\n","        # Observación inicial\n","        return self._obs(), {}\n","\n","\n","    def step(self, action):\n","        #  Datos de t\n","        row = self.df.iloc[self.ptr]\n","\n","        # Cobro dividendos y precios de hoy\n","        divs = np.array([row[f\"dividendo_{t}\"] for t in self.tickers], dtype=np.float32)\n","        self.cash += np.dot(self.port, divs)\n","        prices_today = np.array([row[f\"adj_close_{t}\"] for t in self.tickers], dtype=np.float32)\n","\n","        # Estado preejecución\n","        port_pre = self.port.copy()\n","        cash_pre = self.cash\n","        total_value_pre = cash_pre + np.dot(port_pre, prices_today)\n","\n","        # Fase Warm-UP\n","        a = action.copy()\n","\n","        if self.episode_count <= self.warmup_episodes:\n","            p_override = 0.5\n","            rsi_buy_thr = 30\n","            rsi_sell_thr = 70\n","            for i, t in enumerate(self.tickers):\n","                if np.random.rand() > p_override:\n","                    continue\n","\n","                act = a[i]\n","                rsi = row[f\"tecn_rsi_14_{t}\"]\n","                macd = row[f\"tecn_macd_{t}\"]\n","                macds = row[f\"tecn_macds_{t}\"]\n","                price = row[f\"adj_close_{t}\"]\n","                ma10 = row[f\"ma10_{t}\"]\n","                ma50 = row[f\"ma50_{t}\"]\n","                bb_up = row[f\"bb_up_{t}\"]\n","                bb_dn = row[f\"bb_dn_{t}\"]\n","                fng = row[\"macro_Fear_Greed\"]\n","\n","                # RSI contracíclico + MACD alcista -> compra parcial\n","                if act in (0,1,2) and rsi < rsi_buy_thr and macd > macds:\n","                    a[i] = 3  # se compra 50%\n","                # RSI sobrecompra + MACD bajista -> venta parcial\n","                elif act in (2,3,4) and rsi > rsi_sell_thr and macd < macds:\n","                    a[i] = 1  # se vende 50%\n","                # Bollinger breakout\n","                elif price > bb_up:\n","                    a[i] = 3\n","                elif price < bb_dn:\n","                    a[i] = 1\n","                # Crossover MA10/MA50\n","                elif ma10 is not np.isnan(ma10) and ma50 is not np.isnan(ma50):\n","                    if ma10 > ma50:\n","                        a[i] = 3\n","                    elif ma10 < ma50:\n","                        a[i] = 1\n","                # Fear&Greed extremo\n","                elif fng < 30:\n","                    a[i] = 3\n","                elif fng > 70:\n","                    a[i] = 1\n","\n","        #  Ejecución real de accion\n","\n","        # ventas reales\n","        for i, act in enumerate(a):\n","            if act == 1:    qty = 0.5 * self.port[i]\n","            elif act == 2:  qty = self.port[i]\n","            else:           qty = 0.0\n","            self.cash += qty * prices_today[i] * (1-self.fee)\n","            self.port[i] -= qty\n","\n","        # compras reales\n","        buys = np.where(np.isin(a, [3,4]))[0]\n","        if buys.size>0:\n","            share = self.cash / buys.size\n","            for i in buys:\n","                ratio = 0.5 if a[i]==3 else 1.0\n","                budget = share * ratio\n","                price = prices_today[i]*(1+self.fee)\n","                qty = budget / price\n","                self.port[i] += qty\n","                self.cash    -= qty*price\n","\n","        self.actions_memory.append(a.copy())\n","\n","        # Avance a t+1 y cálculo del valor real\n","        row_next = self.df.iloc[self.ptr + 1]\n","        prices_next = np.array([row_next[f\"adj_close_{t}\"] for t in self.tickers], dtype=np.float32)\n","        divs_next = np.array([row_next[f\"dividendo_{t}\"] for t in self.tickers], dtype=np.float32)\n","        portfolio_value_real = np.dot(self.port, prices_next)\n","        total_value_real = portfolio_value_real + self.cash + self.port.dot(divs_next)\n","        self.asset_memory.append(total_value_real)\n","        self.date_memory.append(self.df.index[self.ptr+1])\n","\n","        # Valor Ideal sin acciones erroneas\n","        port_ideal = port_pre.copy()\n","        cash_ideal = cash_pre\n","\n","        changes = (prices_next - prices_today) / (prices_today + 1e-8)\n","\n","        thr_exec = max(self.fee, self.threshold_norentable)\n","\n","        real_inds = []\n","        for i in range(self.n):\n","            if a[i] in (1,2):\n","                if abs(changes[i]) <= thr_exec:\n","                    a[i] = 0\n","                else:\n","                    real_inds.append(i)\n","            elif a[i] in (3,4):\n","                if abs(changes[i]) <= thr_exec:\n","                    a[i] = 0\n","                else:\n","                    real_inds.append(i)\n","\n","        missed_hold = []\n","        for i in range(self.n):\n","            if a[i] == 0 and abs(changes[i]) > self.threshold_oportunidad:\n","                missed_hold.append(i)\n","\n","        # Ideal Ventas reales rentables y oportunidades perdidas\n","        for i in real_inds:\n","            if a[i] in (1,2) and changes[i]<0:\n","                qty = (0.5 if a[i]==1 else 1.0) * port_pre[i]\n","                cash_ideal += qty * prices_today[i] * (1 - self.fee)\n","                port_ideal[i] -= qty\n","        for i in missed_hold:\n","            if changes[i]<0:\n","                qty = port_pre[i]\n","                cash_ideal += qty * prices_today[i] * (1 - self.fee)\n","                port_ideal[i] = 0\n","\n","        # Ideal de compras reales rentables y oportunidades perdidas\n","        buy_ideal = []\n","        for i in real_inds:\n","            if a[i] in (3,4) and changes[i]>0:\n","                buy_ideal.append(i)\n","        for i in missed_hold:\n","            if changes[i]>0:\n","                buy_ideal.append(i)\n","\n","        if buy_ideal:\n","            share_ideal = cash_ideal / len(buy_ideal)\n","            for i in buy_ideal:\n","                price_i = prices_today[i] * (1 + self.fee)\n","                qty = (0.5 if a[i]==3 else 1.0) * share_ideal / price_i\n","                port_ideal[i] += qty\n","                cash_ideal   -= qty * price_i\n","\n","        total_value_ideal = port_ideal.dot(prices_next) + cash_ideal + port_ideal.dot(divs_next)\n","\n","        # Profit y penalización\n","        profit_real = (total_value_real - total_value_pre)/ (total_value_pre+1e-8)\n","        profit_ideal = (total_value_ideal - total_value_pre)/ (total_value_pre+1e-8)\n","        hold_penalty = max(profit_ideal - profit_real, 0.0)\n","        profit_adj = profit_real - hold_penalty * self.penalty_factor\n","\n","        self.history.append(total_value_real)\n","\n","        # Reward Risk\n","        window = self.history[-30:]\n","        returns = np.diff(window) / window[:-1]\n","        if returns.size > 0:\n","            risk = np.std(returns)\n","        else:\n","            risk = 0.0\n","        risk_reward = 1.0 / (1.0 + risk)\n","\n","        # Reward ESG\n","        holdings_value = self.port * prices_next\n","        weights = holdings_value / (portfolio_value_real + 1e-8)\n","\n","        esg_scores_list = []\n","        for ticker in self.tickers:\n","            score = row_next[f\"esg_score_{ticker}\"]\n","            esg_scores_list.append(score)\n","        esg_scores = np.array(esg_scores_list, dtype=np.float32)\n","\n","        esg_reward = float((weights * esg_scores).sum())\n","\n","        # Vector de recompensa\n","        r_vec = np.array([profit_adj, risk_reward, esg_reward], dtype=np.float32)\n","        self.reward_vec_memory.append(r_vec.copy())\n","        self.ptr += 1\n","\n","        info = {\"reward_components\": r_vec}\n","\n","        terminated = (self.ptr >= len(self.df) - 1) or (total_value_real < 0.01 * self.cash0)\n","        truncated = (self.max_len is not None) and (self.ptr >= self.start + self.max_len)\n","\n","        return self._obs(), r_vec, terminated, truncated, info\n","\n","\n","    def _obs(self):\n","        row = self.df.iloc[self.ptr]\n","\n","        logret = row.filter(regex=r\"^logret_\").values.astype(np.float32)\n","        tech = row.filter(regex=r\"^tecn_\").values.astype(np.float32)\n","        fund = row.filter(regex=r\"^fund_\").values.astype(np.float32)\n","        divs = row.filter(regex=r\"^dividendo_\").values.astype(np.float32)\n","        esg = row.filter(regex=r\"^esg_score_\").values.astype(np.float32)\n","        macro = row.filter(regex=r\"^macro_\").values.astype(np.float32)\n","\n","        prices = np.array([row[f\"adj_close_{t}\"] for t in self.tickers],dtype=np.float32)\n","        pv = np.dot(self.port, prices)\n","        tv = pv + self.cash\n","        weights= (self.port*prices)/(tv or 1.0)\n","        cash_pct = np.array([self.cash/(tv or 1.0)],dtype=np.float32)\n","\n","        return np.concatenate([logret, tech, fund, divs, esg, macro, weights, cash_pct]).astype(np.float32)\n","\n","    #  Métodos para exportar DataFrames\n","    def save_asset_memory(self) -> pd.DataFrame:\n","        \"\"\"Devuelve DataFrame con columnas 'date','asset_value' \"\"\"\n","        return pd.DataFrame({\n","            'date':        self.date_memory,\n","            'asset_value': self.asset_memory\n","        }).set_index('date')\n","\n","    def save_action_memory(self) -> pd.DataFrame:\n","        \"\"\"Devuelve DataFrame de acciones, índice fecha, columnas por ticker \"\"\"\n","        df_act = pd.DataFrame(\n","            self.actions_memory,\n","            index=self.date_memory[:-1],\n","            columns=[f\"action_{t}\" for t in self.tickers]\n","        )\n","        return df_act\n","\n","    def save_reward_vec_memory(self) -> pd.DataFrame:\n","        \"\"\"Devuelve DataFrame con columnas 'date','retorno','riesgo','esg' \"\"\"\n","        df = pd.DataFrame(\n","            self.reward_vec_memory,\n","            index=self.date_memory[1:],\n","            columns=[\"retorno\",\"riesgo\",\"esg\"]\n","        )\n","        return df\n","\n","class ScalarizerWrapper(gym.Wrapper):\n","    \"\"\"\n","    Env wrapper que agrega vector de preferencias w, normaliza y escalariza recompensas multiobjetivo.\n","    Parámetros:\n","      - env: entorno base.\n","      - mode: \"curriculum\" o \"fixed\" para seleccionar estrategias de w.\n","      - fixed_w: vector de pesos ω fijo (None → usa [1,0,0]).\n","      - method: método de escalarización (\"linear\",\"product\",\"chebyshev\",\"soft-penalty\",\"minmax-linear\").\n","      - epsilon, alpha, tau_r, tau_e, z_star, min_w0: parámetros específicos de cada método.\n","      - N0,N1,N2,N3, alpha_small, alpha_mod: parámetros de curriculum.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        env: gym.Env,\n","        mode: str = \"curriculum\",\n","        fixed_w: np.ndarray | None = None,\n","        method: str = \"linear\",\n","        epsilon: float = 1e-6,           # parametros escalarizacion productos. valor mínimo para r⁺\n","        alpha: float = 1.0,              # parametros escalarizacion productos. dureza penalizacion perdida\n","        tau_r: float = 0.6,              # parametros escalarizacion soft-penalty. umbral riesgo\n","        tau_e: float = 0.76,             # parametros escalarizacion soft-penalty. umbral esg\n","        z_star: np.ndarray | None = None,# parametros escalarizacion chebichev. utopia\n","        min_w0: float = 1e-3,            # peso mínimo para w_0\n","        N0: int = 600, N1: int = 1100, N2: int = 1500, N3: int = 1800, # parametros curriculum. limites fases\n","        alpha_small: float = 100.0, alpha_mod: float = 5.0             # parametros curriculum. dispersion\n","    ):\n","\n","        super().__init__(env)\n","        assert mode in (\"curriculum\", \"fixed\"), \"mode debe ser 'curriculum' o 'fixed'\"\n","        self.mode = mode\n","        self.method = method\n","        self.reward_dim = env.reward_dim\n","\n","        self.rewards_memory = []    # lista de rewards escalares por paso\n","        self.w_pref_memory = []     # lista de vectores w por paso\n","\n","        self.min_w0 = min_w0\n","\n","        # w fijo por defecto (1,0,0)\n","        if fixed_w is None:\n","            vec = np.zeros(self.reward_dim, dtype=np.float32)\n","            vec[0] = 1.0\n","            self.fixed_w = vec\n","        else:\n","            self.fixed_w = fixed_w.astype(np.float32)\n","\n","        # curriculum params\n","        self.N0, self.N1, self.N2, self.N3 = N0, N1, N2, N3\n","        self.alpha_small, self.alpha_mod = alpha_small, alpha_mod\n","        self._base_ws = [\n","            np.array([1, 0, 0],np.float32),\n","            np.array([0.5, 0.5, 0],np.float32),\n","            np.array([0.5, 0.35, 0.15],np.float32),\n","            np.array([0.5, 0, 0.5],np.float32),\n","            np.array([0.5, 0.15, 0.35],np.float32),\n","            np.array([0.5, 0.25, 0.25],np.float32),\n","            np.array([1/3, 1/3, 1/3],np.float32),\n","        ]\n","\n","        # parámetros para product+exp\n","        self.epsilon = float(epsilon)\n","        self.alpha = float(alpha)\n","\n","        # init parametros normalización\n","        self.norm_decay = 0.99\n","        self.ema_mean = np.zeros(self.reward_dim, dtype=np.float32)\n","        self.ema_var = np.ones(self.reward_dim, dtype=np.float32)\n","        self.running_min = np.full(self.reward_dim, np.inf, dtype=np.float32)\n","        self.running_max = np.full(self.reward_dim, -np.inf, dtype=np.float32)\n","        self.running_min[2] = 0.0\n","        self.running_max[2] = 1.0\n","        self.tau_r = tau_r;\n","        self.tau_e = tau_e;\n","\n","        # z_star initialization\n","        if z_star is not None:\n","            self.z_star = z_star.astype(np.float32)\n","        elif method==\"chebyshev\":\n","            self.z_star = np.array([1.0,0.0,0.85],dtype=np.float32)\n","        else:\n","            self.z_star = np.ones(self.reward_dim,dtype=np.float32)\n","\n","        # ampliar observation_space + reward_dim dims\n","        low = np.concatenate([env.observation_space.low,np.zeros(self.reward_dim, dtype=np.float32)])\n","        high = np.concatenate([env.observation_space.high,np.ones (self.reward_dim, dtype=np.float32)])\n","        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n","\n","    def _choose_w(self):\n","        \"\"\"\n","        Devuelve vector w según modo:\n","          - Si mode=\"fixed\", retorna fixed_w.\n","          - Si mode=\"curriculum\", genera w siguiendo fases N0–N3 con base en _base_ws.\n","        \"\"\"\n","        if self.mode == \"fixed\":\n","            return self.fixed_w\n","\n","        # else curriculum:\n","        epi = self.env.episode_count\n","        if   epi <= self.N0:\n","           w =  self.fixed_w\n","        elif epi <= self.N1:\n","            idx = (epi - self.N0 - 1) % len(self._base_ws)\n","            w =  self._base_ws[idx]\n","        elif epi <= self.N2:\n","            idx = (epi - self.N0 - 1) % len(self._base_ws)\n","            base = self._base_ws[idx]\n","            alpha= base * self.alpha_small + 1e-6\n","            w =  np.random.dirichlet(alpha).astype(np.float32)\n","        elif epi <= self.N3:\n","            idx = (epi - self.N0 - 1) % len(self._base_ws)\n","            base = self._base_ws[idx]\n","            alpha= base * self.alpha_mod + 1e-6\n","            w = np.random.dirichlet(alpha).astype(np.float32)\n","        else:\n","            w =  np.random.dirichlet(np.ones(self.reward_dim,dtype=np.float32)).astype(np.float32)\n","\n","        # el mínimo en la componente 0 y normalización:\n","        w[0] = max(w[0], self.min_w0)\n","        w = w / w.sum()\n","\n","        return w.astype(np.float32)\n","\n","    def _normalize(self, r_vec: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Normaliza r_vec según method:\n","          - \"linear\"/\"product\": z-score o recorte por desviación.\n","          - \"chebyshev\"/\"soft-penalty\"/\"minmax-linear\": escala a [0,1] usando mínimos y máximos acumulados.\n","        Parámetros:\n","          - r_vec: vector de recompensas bruto de dimensión reward_dim.\n","        \"\"\"\n","        if self.method in (\"linear\", \"product\"):\n","            delta = r_vec - self.ema_mean\n","            self.ema_mean += (1 - self.norm_decay) * delta\n","            self.ema_var = self.norm_decay * self.ema_var + (1 - self.norm_decay) * (delta ** 2)\n","            std = np.sqrt(self.ema_var + 1e-8)\n","\n","            if self.method == \"linear\":\n","                return (r_vec - self.ema_mean) / std  # z-score\n","            else:  # \"product\"\n","                return np.clip(r_vec / std, 1e-6, None) # sólo escalar y recortar\n","\n","        elif self.method in (\"chebyshev\", \"soft-penalty\",\"minmax-linear\"):\n","            self.running_min = np.minimum(self.running_min, r_vec)\n","            self.running_max = np.maximum(self.running_max, r_vec)\n","            denom = (self.running_max - self.running_min) + 1e-8\n","            return np.clip((r_vec - self.running_min)/denom, 0.0, 1.0)\n","\n","        else:\n","            raise ValueError(f\"Unknown normalize method {self.method!r}\")\n","\n","    def scalarize(self, r_vec: np.ndarray) -> float:\n","        \"\"\"\n","        Calcula recompensa escalar a partir de r_vec:\n","          - Normaliza y aplica:\n","            • \"linear\": producto punto con w.\n","            • \"product\": producto ponderado con penalización exponencial.\n","            • \"chebyshev\": -max(w * |r_norm - z_star|).\n","            • \"soft-penalty\": w[0]*profit - w[1]*penalty_r - w[2]*penalty_e.\n","            • \"minmax-linear\": igual que \"linear\" tras normalizar.\n","        Parámetros:\n","          - r_vec: vector de recompensas (profit, riesgo, esg).\n","        \"\"\"\n","        if self.method in (\"linear\",\"product\",\"chebyshev\",\"soft-penalty\",\"minmax-linear\"):\n","            r_norm = self._normalize(r_vec)\n","\n","        if self.method in (\"linear\", \"minmax-linear\"):\n","            return float(np.dot(self.w, r_norm))\n","\n","        elif self.method == \"product\": # Producto con penalización exponencial de pérdidas\n","\n","            # producto de las partes positivas (o epsilon si r<0)\n","            P = 1.0\n","            for i in range(self.reward_dim):\n","                r_i = r_norm[i]\n","                factor = r_i if r_i > 0 else self.epsilon\n","                P *= factor ** self.w[i]\n","\n","            # factor exp para las pérdidas\n","            loss = sum(self.w[i] * max(-r_norm[i], 0.0) for i in range(self.reward_dim))\n","            Q = np.exp(- self.alpha * loss)\n","            return float(P * Q)\n","\n","        elif self.method == \"chebyshev\":\n","            return float(-np.max(self.w * np.abs(r_norm - self.z_star)))\n","\n","        elif self.method == \"soft-penalty\":\n","            profit_n, risk_n, esg_n = r_norm\n","            penalty_r = max(0.0, self.tau_r - risk_n)\n","            penalty_e = max(0.0, self.tau_e - esg_n)\n","            return float( self.w[0] * profit_n - self.w[1] * penalty_r - self.w[2] * penalty_e)\n","\n","        else:\n","            raise ValueError(f\"Unknown method {self.method!r}\")\n","\n","    def reset(self, **kwargs):\n","        \"\"\"\n","        Resetea el entorno base, elige w inicial y concatena w a la observación.\n","        Retorna:\n","          - obs: observación ampliada [obs_base, w].\n","          - info: diccionario de información adicional.\n","        \"\"\"\n","        obs_base, info = self.env.reset(**kwargs)\n","        self.w = self._choose_w()\n","        self.w_pref_memory = [ self.w.copy() ]\n","        self.rewards_memory = []\n","        return np.concatenate([obs_base, self.w]), info\n","\n","    def step(self, action):\n","        \"\"\"\n","        Realiza un paso en el entorno base, conserva r_vec, calcula r escalar,\n","        actualiza w_pref_memory y devuelve observación, recompensa escalar, done, truncated e info.\n","        Parámetros:\n","          - action: acción enviada al entorno base.\n","        Retorna:\n","          - obs: nueva observación ampliada.\n","          - r: recompensa escalar.\n","          - term_env, trunc_env: flags de terminación.\n","          - info: diccionario con \"w_pref\" y \"reward_components\".\n","        \"\"\"\n","        obs_base, r_vec, term_env, trunc_env, info = self.env.step(action)\n","        self.w_pref_memory.append(self.w.copy())\n","        obs = np.concatenate([obs_base, self.w])\n","        r = self.scalarize(r_vec)\n","        self.rewards_memory.append(r)\n","        info[\"w_pref\"] = self.w\n","        return obs, r, term_env, trunc_env, info\n","\n","    def save_w_pref_memory(self) -> pd.DataFrame:\n","        \"\"\"\n","        Devuelve DataFrame con historiales de vectores w_pref por paso.\n","        \"\"\"\n","        df = pd.DataFrame(\n","            self.w_pref_memory,\n","            index=self.env.date_memory[:-1],\n","            columns=[f\"w_ret\",\"w_risk\",\"w_esg\"]\n","        )\n","        return df\n","\n","    def save_scalar_reward_memory(self) -> pd.DataFrame:\n","        \"\"\"\n","        Devuelve DataFrame con recompensas escalares por paso.\n","        \"\"\"\n","        return pd.DataFrame({\n","            'date':   self.env.date_memory[1:],\n","            'reward': self.rewards_memory\n","        }).set_index('date')\n","\n","\n","# ------------- 5. Custom policy ---------------------------------------\n","class CustomFeatureExtractorBasic(BaseFeaturesExtractor):\n","    \"\"\"\n","    Extractor de características sencillo con dos capas lineales.\n","    Parámetros:\n","      - observation_space: espacio de observación del entorno.\n","      - features_dim: dimensión de salida de las características.\n","    \"\"\"\n","\n","    def __init__(self, observation_space, features_dim=128):\n","        super().__init__(observation_space, features_dim)\n","        input_dim = observation_space.shape[0]\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 256), nn.ReLU(),\n","            nn.Linear(256, features_dim), nn.ReLU()\n","        )\n","    def forward(self, observations: th.Tensor) -> th.Tensor:\n","        return self.net(observations)\n","\n","\n","class CustomFeatureExtractor(BaseFeaturesExtractor):\n","    \"\"\"\n","    Extractor de características profundo con 4 capas lineales.\n","    Parámetros:\n","      - observation_space: espacio de observación del entorno.\n","      - features_dim: dimensión de salida de las características.\n","    \"\"\"\n","\n","    def __init__(self, observation_space, features_dim=128):\n","        super().__init__(observation_space, features_dim)\n","        input_dim = observation_space.shape[0]\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 512), nn.ReLU(),\n","            nn.Linear(512, 256), nn.ReLU(),\n","            nn.Linear(256, 128), nn.ReLU(),\n","            nn.Linear(128, features_dim), nn.ReLU()\n","        )\n","    def forward(self, observations: th.Tensor) -> th.Tensor:\n","        return self.net(observations)\n","\n","\n","class CustomMlpPolicy(ActorCriticPolicy):\n","    \"\"\"\n","    Política Actor-Crítico que utiliza CustomFeatureExtractor para obtener representaciones de entrada de tamaño 128.\n","    Parámetros:\n","      - args, kwargs: argumentos de ActorCriticPolicy.\n","    \"\"\"\n","\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs,\n","                         features_extractor_class=CustomFeatureExtractor,\n","                         features_extractor_kwargs={\"features_dim\":128})\n","\n","# ------------- 6. Callbacks -------------------------------------------\n","\n","class TensorboardCallback(BaseCallback):\n","    \"\"\"\n","    Acumula componentes de recompensa por episodio y registra su EMA en TensorBoard.\n","    Parámetros:\n","      - ema_alpha: factor de suavizado exponencial para EMA de recompensas.\n","    \"\"\"\n","\n","    def __init__(self, ema_alpha=0.01, verbose=0):\n","        super().__init__(verbose)\n","        self.writer = None\n","        self.ema_alpha = ema_alpha\n","\n","        # Acumuladores por episodio\n","        self.ep_r0 = 0.0  # retorno\n","        self.ep_r1 = 0.0  # riesgo\n","        self.ep_r2 = 0.0  # esg\n","\n","        # EMA por componente\n","        self.ema_r0 = None\n","        self.ema_r1 = None\n","        self.ema_r2 = None\n","\n","    def _on_training_start(self) -> None:\n","        logdir = self.model.tensorboard_log or \"runs\"\n","        self.writer = SummaryWriter(log_dir=logdir)\n","\n","    def _on_step(self) -> bool:\n","        infos = self.locals.get(\"infos\", [])\n","        dones = self.locals.get(\"dones\", [])\n","        step = self.num_timesteps\n","\n","        # Acumulación por paso\n","        for info in infos:\n","            if \"reward_components\" in info:\n","                r = info[\"reward_components\"]\n","                self.ep_r0 += r[0]\n","                self.ep_r1 += r[1]\n","                self.ep_r2 += r[2]\n","                break\n","\n","        # Al terminar episodio, actualiza y loguea EMA\n","        for done in dones:\n","            if done:\n","                if self.ema_r0 is None:\n","                    self.ema_r0, self.ema_r1, self.ema_r2 = self.ep_r0, self.ep_r1, self.ep_r2\n","                else:\n","                    self.ema_r0 = self.ema_alpha * self.ep_r0 + (1 - self.ema_alpha) * self.ema_r0\n","                    self.ema_r1 = self.ema_alpha * self.ep_r1 + (1 - self.ema_alpha) * self.ema_r1\n","                    self.ema_r2 = self.ema_alpha * self.ep_r2 + (1 - self.ema_alpha) * self.ema_r2\n","\n","                self.writer.add_scalar(\"ep_ema/retorno\", self.ema_r0, step)\n","                self.writer.add_scalar(\"ep_ema/riesgo\",  self.ema_r1, step)\n","                self.writer.add_scalar(\"ep_ema/esg\",     self.ema_r2, step)\n","\n","                self.ep_r0 = self.ep_r1 = self.ep_r2 = 0.0\n","\n","        return True\n","\n","class ScaledEvalCallback(EvalCallback):\n","    \"\"\"\n","    Extiende EvalCallback para ajustar los límites de currículum\n","    N0, N1, N2 y N3 en función de la frecuencia de evaluación.\n","    Parámetros:\n","      - eval_env: entorno de evaluación.\n","      - eval_freq: pasos entre evaluaciones.\n","      - n_eval_episodes: episodios por evaluación.\n","      - best_model_save_path, log_path: rutas de guardado.\n","      - max_episode_length: longitud fija de episodios.\n","      - base_N: tupla (N0,N1,N2,N3) originales.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        eval_env,\n","        eval_freq: int = 20480,\n","        n_eval_episodes: int = 5,\n","        best_model_save_path: str = \"best_model\",\n","        log_path: str = \"best_model\",\n","        verbose: int = 1,\n","        max_episode_length: int = 512,\n","        base_N: tuple[int, int, int, int] = (600, 1100, 1500, 1800),\n","        **kwargs\n","    ):\n","        \"\"\"\n","        :param eval_env: el VecEnv de evaluación\n","        :param eval_freq: frecuencia (en pasos) de la evaluación\n","        :param n_eval_episodes: episodios por evaluación\n","        :param best_model_save_path: dónde guardar el mejor modelo\n","        :param log_path: dónde escribir los logs de evaluación\n","        :param max_episode_length: longitud fija de cada episodio\n","        :param base_N: (N0, N1, N2, N3) originales de tu ScalarizerWrapper\n","        :param kwargs: resto de args para EvalCallback\n","        \"\"\"\n","        super().__init__(\n","            eval_env,\n","            eval_freq=eval_freq,\n","            n_eval_episodes=n_eval_episodes,\n","            best_model_save_path=best_model_save_path,\n","            log_path=log_path,\n","            verbose=verbose,\n","            **kwargs\n","        )\n","        self.max_episode_length = max_episode_length\n","        self.base_N0, self.base_N1, self.base_N2, self.base_N3 = base_N\n","\n","    def _on_step(self) -> bool:\n","        # Calcula cuántos episodios cabían entre evaluaciones\n","        eps_per_eval = self.eval_freq / self.max_episode_length\n","        eps_per_eval = max(eps_per_eval, 1.0)\n","\n","        # Escala cada N en proporción\n","        scaled_N0 = int(self.base_N0 / eps_per_eval)\n","        scaled_N1 = int(self.base_N1 / eps_per_eval)\n","        scaled_N2 = int(self.base_N2 / eps_per_eval)\n","        scaled_N3 = int(self.base_N3 / eps_per_eval)\n","\n","        # Extrae el ScalarizerWrapper de dentro de VecMonitor→VecNormalize→DummyVecEnv\n","        env = self.eval_env\n","\n","        # Si está envuelto en VecMonitor o VecNormalize\n","        while hasattr(env, \"env\"):\n","            env = env.env\n","\n","        # DummyVecEnv guarda la lista .envs\n","        for sub in getattr(env, \"envs\", []):\n","            # si hay un Monitor\n","            real_env = getattr(sub, \"env\", sub)\n","            if hasattr(real_env, \"N0\"):\n","                real_env.N0, real_env.N1, real_env.N2, real_env.N3 = (\n","                    scaled_N0, scaled_N1, scaled_N2, scaled_N3\n","                )\n","\n","        # EvalCallback original\n","        return super()._on_step()\n","\n","\n","\n","class EntropyAnnealingCallback(BaseCallback):\n","    \"\"\"\n","    Actualiza el coeficiente de entropía del modelo de manera lineal\n","    Parámetros:\n","      - ent_schedule: función que recibe progreso restante y devuelve coeficiente.\n","      - total_timesteps: número total de pasos de entrenamiento.\n","    \"\"\"\n","\n","    def __init__(self, ent_schedule: callable, total_timesteps: int, verbose=0):\n","        super().__init__(verbose)\n","        self.ent_schedule = ent_schedule\n","        self.total_timesteps = total_timesteps\n","\n","    def _on_step(self) -> bool:\n","        progress = 1 - (self.num_timesteps / self.total_timesteps)\n","        new_ent = self.ent_schedule(progress)\n","        self.model.ent_coef = new_ent\n","        return True\n","\n","class CheckpointWithVecNorm(BaseCallback):\n","    \"\"\"\n","    Guarda el modelo y los parámetros de VecNormalize cada `save_freq` pasos en `save_path` con prefijo `prefix`.\n","    Parámetros:\n","      - save_freq: frecuencia de guardado.\n","      - save_path: directorio donde guardar checkpoints.\n","      - prefix: prefijo para los archivos guardados.\n","      - vecnorm: instancia de VecNormalize a guardar.\n","    \"\"\"\n","\n","    def __init__(self, save_freq, save_path, prefix, vecnorm, verbose=0):\n","        super().__init__(verbose)\n","        self.save_freq = save_freq\n","        self.save_path = save_path\n","        self.prefix = prefix\n","        self.vecnorm = vecnorm\n","\n","    def _on_step(self) -> bool:\n","        if self.num_timesteps % self.save_freq == 0:\n","            fname = os.path.join(self.save_path, f\"{self.prefix}_{self.num_timesteps}\")\n","            self.model.save(fname)\n","            self.vecnorm.save(fname + \"_vecnorm\")\n","        return True\n","\n","\n","# ------------- 7. Constructor -------------------------------------------\n","\n","# Constructor de entorno\n","def make_env_constructor(\n","    df: pd.DataFrame,\n","    tickers: list[str],\n","    method: str = \"linear\",\n","    max_len: int | None = 512,\n","    random_start: bool = True,\n","    scalar_mode: str = \"fixed\",\n","    fixed_w: np.ndarray | None = None,\n","    cash: float = 1e6,\n","    fee: float = 0.001\n","):\n","    \"\"\"\n","    Devuelve la función `_init()` que crea un MOTradingEnv envuelto en ScalarizerWrapper.\n","      - df: DataFrame preprocesado indexado por fecha.\n","      - tickers: lista de strings con símbolos de activos.\n","      - method: método de escalarización (\"linear\",\"product\",…).\n","      - max_len: longitud máxima del episodio (None = sin límite).\n","      - random_start: True → arranca en índice aleatorio.\n","      - scalar_mode: \"curriculum\" o \"fixed\".\n","      - fixed_w: array de pesos ω para modo fixed (None → [1,0,0]).\n","      - cash, fee: capital inicial y comisiones.\n","    \"\"\"\n","    def _init():\n","        base = MOTradingEnv(\n","            data_df=df,\n","            tickers=tickers,\n","            cash=cash,\n","            fee=fee,\n","            max_episode_length=max_len,\n","            random_start=random_start\n","        )\n","        if scalar_mode == \"fixed\":\n","            return ScalarizerWrapper(\n","                base,\n","                mode=\"fixed\",\n","                method=method,\n","                fixed_w=(fixed_w.astype(np.float32) if fixed_w is not None else None)\n","            )\n","        else:\n","            return ScalarizerWrapper(base, mode=\"curriculum\", method=method)\n","    return _init\n","\n","\n","\n","# ------------- 8. Evaluacion -------------------------------------------\n","\n","# Función para construir un vec_env normalizado\n","def make_normalized_env(\n","    df: pd.DataFrame,\n","    tickers: List[str],\n","    method: str,\n","    norm_path: str,\n","    max_len: int = 512,\n","    random_start: bool = False,\n","    fixed_w: Optional[np.ndarray] = None\n","):\n","    \"\"\"\n","    Construye un DummyVecEnv que envuelve ScalarizerWrapper (modo fixed) y luego carga\n","    VecNormalize desde norm_path. Si no se pasa fixed_w, usa [1/3,1/3,1/3].\n","    \"\"\"\n","    if fixed_w is None:\n","        fixed_w = np.ones(3, dtype=float) / 3\n","\n","    # Reutiliza make_env_constructor en modo “fixed”\n","    constructor = make_env_constructor(\n","        df=df,\n","        tickers=tickers,\n","        method=method,\n","        max_len=max_len,\n","        random_start=random_start,\n","        scalar_mode=\"fixed\",\n","        fixed_w=fixed_w\n","    )\n","\n","    vec_env = DummyVecEnv([constructor])\n","    vec_env = VecMonitor(vec_env)\n","\n","    # Carga el VecNormalize preentrenado\n","    vec_env = VecNormalize.load(norm_path, vec_env)\n","    vec_env.training    = False\n","    vec_env.norm_reward = False\n","    return vec_env\n","\n","\n","#  Función de backtest\n","def evaluate_agent_backtest(\n","    model,\n","    vec_env,\n","    df: pd.DataFrame,\n","    tickers: List[str],\n","    cash: float = 1e6,\n","    deterministic: bool = True\n","):\n","    \"\"\"\n","    Usa el vec_env ya normalizado para hacer rollout.\n","    Devuelve df_daily y métricas.\n","    \"\"\"\n","    obs = vec_env.reset()\n","    total_trades = 0\n","    records = []\n","    done = False\n","\n","    inner = vec_env.venv.venv.envs[0].env  # VecNormalize → VecMonitor → DummyVecEnv → ScalarizerWrapper → MOTradingEnv\n","\n","    while not done:\n","        t = inner.ptr\n","        date = df.index[t]\n","        prices = df.iloc[t][[f\"adj_close_{tk}\" for tk in tickers]].values\n","        tv_pre = inner.cash + (inner.port * prices).sum()\n","\n","        action, _ = model.predict(obs, deterministic=deterministic)\n","        trades = int((action != 0).sum())\n","        total_trades += trades\n","\n","        obs, _, dones, infos = vec_env.step(action)\n","        done = dones[0]\n","        info = infos[0]\n","\n","        # post-step\n","        t2 = inner.ptr\n","        prices2 = df.iloc[t2][[f\"adj_close_{tk}\" for tk in tickers]].values\n","        tv_post = inner.cash + (inner.port * prices2).sum()\n","        ret = (tv_post - tv_pre) / tv_pre\n","\n","        # calculo ESG\n","        try:\n","            esg_scores = df.iloc[t][[f\"esg_score_{tk}\" for tk in tickers]].values\n","            weights = (inner.port * prices) / tv_pre\n","            esg_port = float((weights * esg_scores).sum())\n","        except KeyError:\n","            esg_port = None\n","\n","        # registro\n","        records.append({\n","            \"date\": date,\n","            \"portfolio_value\": tv_pre,\n","            \"daily_return\": ret,\n","            \"n_trades\": trades,\n","            \"esg_portfolio\": esg_port\n","\n","        })\n","\n","    df_daily = pd.DataFrame(records).set_index(\"date\")\n","\n","    # cálculo de métricas\n","    ini = cash\n","    fin = df_daily[\"portfolio_value\"].iloc[-1]\n","    roi = fin/ini - 1\n","    days = (df_daily.index[-1] - df_daily.index[0]).days\n","    years = days/365\n","    cagr = (fin/ini)**(1/years) - 1\n","    vol_ann = df_daily[\"daily_return\"].std() * np.sqrt(252)\n","    sharpe = df_daily[\"daily_return\"].mean() / df_daily[\"daily_return\"].std() * np.sqrt(252)\n","    dd = (df_daily[\"portfolio_value\"] - df_daily[\"portfolio_value\"].cummax()) / df_daily[\"portfolio_value\"].cummax()\n","    maxdd = dd.min()\n","    avg_esg = df_daily[\"esg_portfolio\"].dropna().mean() if \"esg_portfolio\" in df_daily else None\n","\n","\n","    metrics = {\n","        \"ROI\": roi,\n","        \"CAGR\": cagr,\n","        \"Vol_Ann\": vol_ann,\n","        \"Sharpe\": sharpe,\n","        \"Max_Drwdwn\": maxdd,\n","        \"final_value\": fin,\n","        \"avg_dly_ret\": df_daily[\"daily_return\"].mean(),\n","        \"avg_dly_vol\": df_daily[\"daily_return\"].std(),\n","        \"avg_esg\": avg_esg,\n","        \"total_trads\": total_trades\n","    }\n","    return df_daily, metrics\n","\n","\n","\n","# Funcion para estrategia Buy & Hold con igual peso\n","def backtest_buy_and_hold(df, tickers, cash=1e6):\n","    \"\"\"\n","    Simula estrategia Buy & Hold con asignación igual, calcula retornos diarios\n","    y puntuación ESG, retorna DataFrame y métricas de rendimiento.\n","    \"\"\"\n","    n = len(tickers)\n","\n","    # precios y valor de cartera\n","    df_prices = df[[f\"adj_close_{tk}\" for tk in tickers]]\n","    alloc = cash / n\n","    df_val = df_prices.div(df_prices.iloc[0]).mul(alloc)\n","    df_val[\"portfolio_value\"] = df_val.sum(axis=1)\n","    df_daily = pd.DataFrame({\n","        \"date\": df_val.index,\n","        \"portfolio_value\": df_val[\"portfolio_value\"]\n","    }).set_index(\"date\")\n","\n","    # número de acciones compradas en t0\n","    shares = { tk: alloc / df_prices[f\"adj_close_{tk}\"].iloc[0]\n","               for tk in tickers }\n","\n","    # returns diarios\n","    df_daily[\"daily_return\"] = df_daily[\"portfolio_value\"].pct_change().fillna(0)\n","\n","    # esg_portfolio en cada fecha\n","    esg_port = []\n","    for date, row in df.iterrows():\n","        pos_vals = np.array([ shares[tk] * row[f\"adj_close_{tk}\"]\n","                              for tk in tickers ])\n","        port_val = pos_vals.sum()\n","        weights = pos_vals / port_val\n","        scores = np.array([ row[f\"esg_score_{tk}\"] for tk in tickers ])\n","        esg_port.append((weights * scores).sum())\n","    df_daily[\"esg_portfolio\"] = esg_port\n","\n","    # métricas\n","    ini, fin = cash, df_daily[\"portfolio_value\"].iloc[-1]\n","    roi = fin/ini - 1\n","    days = (df_daily.index[-1] - df_daily.index[0]).days\n","    years = days/365\n","    cagr = (fin/ini)**(1/years) - 1\n","    final_value = df_daily[\"portfolio_value\"].iloc[-1]\n","    ret = df_daily[\"daily_return\"].dropna()\n","    vol_ann = ret.std()*np.sqrt(252)\n","    sharpe = ret.mean()/ret.std()*np.sqrt(252)\n","    dd = (df_daily[\"portfolio_value\"]-df_daily[\"portfolio_value\"].cummax())/df_daily[\"portfolio_value\"].cummax()\n","    maxdd = dd.min()\n","\n","    avg_dly_ret = ret.mean()\n","    avg_dly_vol = ret.std()\n","    avg_esg = df_daily[\"esg_portfolio\"].mean()\n","    total_trads = 12\n","\n","    metrics = {\n","        \"ROI\":            roi,\n","        \"CAGR\":           cagr,\n","        \"Vol_Ann\":        vol_ann,\n","        \"Sharpe\":         sharpe,\n","        \"Max_Drwdwn\":     maxdd,\n","        \"final_value\":    final_value,\n","        \"avg_dly_ret\":    avg_dly_ret,\n","        \"avg_dly_vol\":    avg_dly_vol,\n","        \"avg_esg\":        avg_esg,\n","        \"total_trads\":    total_trads\n","    }\n","    return df_daily, metrics\n","\n","\n","# Funcion para calculo de Benchmark ETFs\n","def fetch_etf(ticker, start, end):\n","    \"\"\"\n","    Descarga precios ajustados de un ETF usando yfinance y devuelve\n","    DataFrame con la columna renombrada como `ticker`.\n","    \"\"\"\n","\n","    df = yf.download(ticker, start=start, end=end, auto_adjust=False)[[\"Adj Close\"]]\n","    df = df.rename(columns={\"Adj Close\":ticker})\n","    return df\n","\n","def backtest_etfs(etf_tickers: List[str], start: str, end: str, cash: float=1e6):\n","    \"\"\"\n","    Backtest de varios ETFs entre dos fechas. Devuelve\n","    DataFrame de valores diarios y diccionario con métricas por cada ETF.\n","    \"\"\"\n","\n","    df_list = []\n","    metrics = {}\n","    for tk in etf_tickers:\n","\n","        df = yf.download(tk, start=start, end=end, auto_adjust=False)[[\"Adj Close\"]]\n","        df = df.dropna()\n","\n","        val = df[\"Adj Close\"] / df[\"Adj Close\"].iloc[0] * cash\n","        val.name = tk\n","        df_list.append(val)\n","\n","        roi = val.iloc[-1]/val.iloc[0] - 1\n","        days = (val.index[-1] - val.index[0]).days\n","        years = days / 365.0\n","        cagr = (val.iloc[-1]/val.iloc[0])**(1/years) - 1\n","        ret = val.pct_change().dropna()\n","        vol_ann = ret.std() * np.sqrt(252)\n","        sharpe = ret.mean() / ret.std() * np.sqrt(252)\n","        dd = (val - val.cummax()) / val.cummax()\n","        maxdd = dd.min()\n","        final_value = val.iloc[-1]\n","        avg_dly_ret = ret.mean()\n","        avg_dly_vol = ret.std()\n","        avg_esg = None\n","        total_trads = 1\n","\n","        metrics[tk] = {\"ROI\":roi, \"CAGR\":cagr, \"Vol_Ann\":vol_ann, \"Sharpe\":sharpe, \"Max_Drwdwn\":maxdd, \"final_value\": final_value,\n","               \"avg_dly_ret\": avg_dly_ret, \"avg_dly_vol\": avg_dly_vol,\"avg_esg\": avg_esg,\"total_trads\": total_trads}\n","\n","    df_etfs = pd.concat(df_list, axis=1)\n","\n","    return df_etfs, metrics\n","\n","# Funcion para evaluación de robustez\n","def evaluate_robustness(\n","    model_paths: dict[str,str],\n","    norm_paths: dict[str,str],\n","    method_map: dict[str,str],\n","    df: pd.DataFrame,\n","    tickers: list[str],\n","    n_runs: int = 100\n","):\n","    \"\"\"\n","    Ejecuta n_runs backtests estocásticos por agente y devuelve\n","    un dict de la forma: metrics_hist[agent] = {'final_value', 'Vol_Ann', 'avg_esg'}\n","    \"\"\"\n","    metrics_hist = {agent: {'final_value':[], 'Vol_Ann':[], 'avg_esg':[]}\n","                    for agent in model_paths.keys()}\n","\n","    for agent, path in model_paths.items():\n","        print(f\"Evaluando robustez de {agent}...\")\n","        for i in range(n_runs):\n","\n","            # carga del modelo y normalizador\n","            model = PPO.load(path)\n","            method = method_map[agent]\n","            vec_env = make_normalized_env(\n","                df, tickers, method,\n","                norm_path=norm_paths[agent],\n","                max_len=None,\n","                random_start=False,\n","                fixed_w=None\n","            )\n","\n","            # roll‐out\n","            df_daily, met = evaluate_agent_backtest(model, vec_env, df, tickers, deterministic=False)\n","            metrics_hist[agent]['final_value'].append(met['final_value'])\n","            metrics_hist[agent]['Vol_Ann'].append(met['Vol_Ann'])\n","            metrics_hist[agent]['avg_esg'].append(met['avg_esg'])\n","            vec_env.close()\n","    return metrics_hist\n","\n","# Funcion para gráfico de violines\n","def plot_violin_metrics(\n","    metrics_hist: dict[str,dict[str,list]],\n","    agents: list[str],\n","    agent_colors: dict[str,str]\n",") -> plt.Figure:\n","    \"\"\"\n","    Crea 3 violinplots comparando agentes\n","    \"\"\"\n","    TITLE_FS = 24\n","    TICK_FS = 16\n","\n","    labels = agents\n","    data_final = [metrics_hist[a]['final_value'] for a in labels]\n","    data_vol = [metrics_hist[a]['Vol_Ann']     for a in labels]\n","    data_esg = [metrics_hist[a]['avg_esg']     for a in labels]\n","\n","    fig, axes = plt.subplots(1, 3, figsize=(18,5), constrained_layout=True)\n","\n","    for ax, data, title in zip(\n","        axes,\n","        [data_final, data_vol, data_esg],\n","        [\"Valor final de cartera\", \"Volatilidad anualizada\", \"Puntuación media ESG\"]\n","    ):\n","        vp = ax.violinplot(data, showmeans=True)\n","\n","        for i, body in enumerate(vp['bodies']):\n","            lbl = labels[i]\n","            body.set_facecolor(agent_colors[lbl])\n","            body.set_edgecolor('black')\n","            body.set_alpha(0.7)\n","\n","        ax.set_title(title, fontsize=TITLE_FS)\n","        ax.set_xticks(range(1, len(labels)+1))\n","        ax.set_xticklabels(labels, rotation=45, ha=\"right\", fontsize=TICK_FS)\n","        ax.tick_params(axis='y', labelsize=TICK_FS)\n","        ax.grid(True)\n","\n","    return fig\n","\n","# ------------- 9. Frente de Pareto -------------------------------------------\n","\n","def collect_pareto_points(\n","    model,\n","    df: pd.DataFrame,\n","    tickers: list[str],\n","    vecnorm_path: str,\n","    method: str = \"chebyshev\",\n","    ws: list[np.ndarray] = None,\n","):\n","    \"\"\"\n","    Para cada w en `ws`, crea un entorno con esos pesos fijos, carga las estadísticas de normalización,\n","    ejecuta un episodio completo y acumula sus componentes de recompensa. Devuelve una lista de (w, recompensa_total).\n","    \"\"\"\n","    # Estadísticas de VecNormalize ya entrenado\n","    with open(vecnorm_path, \"rb\") as f:\n","        loaded_norm: VecNormalize = pickle.load(f)\n","    obs_rms = loaded_norm.obs_rms\n","    ret_rms = loaded_norm.ret_rms\n","\n","    points = []\n","    for w in ws:\n","\n","        # Constructor w en modo \"fixed\"\n","        constructor = make_env_constructor(\n","            df=df,\n","            tickers=tickers,\n","            method=method,\n","            max_len=None,\n","            random_start=False,\n","            scalar_mode=\"fixed\",\n","            fixed_w=w.astype(np.float32),\n","            cash=1e6,\n","            fee=0.001\n","        )\n","\n","        # Env vectorizado y  VecMonitor\n","        venv = DummyVecEnv([constructor])\n","        venv = VecMonitor(venv)\n","\n","        # Nuevo VecNormalize en modo evaluación con las mismas stats\n","        norm_env = VecNormalize(venv, training=False, norm_reward=False)\n","        norm_env.obs_rms = obs_rms\n","        norm_env.ret_rms = ret_rms\n","\n","        # Ejecuta un episodio completo y acumular reward_components\n","        obs = norm_env.reset()\n","        done = False\n","        vecs = []\n","        while not done:\n","            action, _ = model.predict(obs, deterministic=True)\n","            obs, _, done, infos = norm_env.step(action)\n","            vecs.append(infos[0][\"reward_components\"])\n","\n","        total_vec = np.sum(vecs, axis=0)\n","        points.append((w, total_vec))\n","\n","        norm_env.close()\n","\n","    return points\n","\n","\n","def is_pareto(a: np.ndarray) -> np.ndarray:\n","    \"\"\"Máscara booleana de los puntos no dominados.\"\"\"\n","    dominated = np.zeros(a.shape[0], dtype=bool)\n","    for i, pi in enumerate(a):\n","        if dominated[i]: continue\n","        # un punto j domina a i si pj >= pi en todas y > en alguna\n","        wins = np.all(a >= pi, axis=1) & np.any(a > pi, axis=1)\n","        dominated |= wins\n","        dominated[i] = False\n","    return ~dominated\n","\n","\n","# ------------- Linear annealing schedule ------------------------------\n","def linear_schedule(initial_value: float, final_value: float) -> callable:\n","    \"\"\"\n","    Devuelve una función que interpola linealmente un valor de `initial_value` a `final_value` según el progreso restante.\n","    \"\"\"\n","    def func(progress_remaining: float) -> float:\n","        return final_value + (initial_value - final_value) * progress_remaining\n","    return func\n"]}]}